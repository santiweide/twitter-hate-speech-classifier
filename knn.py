import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import Trainer, TrainingArguments
from datasets import Dataset
from pathlib import Path
from scipy.special import softmax
import urllib.request
import csv
import kagglehub  # å¯¼å…¥ kagglehub
import sys        # å¯¼å…¥ sys ä»¥ä¾¿åœ¨æ–‡ä»¶æœªæ‰¾åˆ°æ—¶é€€å‡º

# --- 1. ä»Žä½ çš„ä»£ç ä¸­åŠ è½½é¢„å¤„ç†å‡½æ•°å’Œæ¨¡åž‹ ---

# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []
    for t in text.split(" "):
        t = '@user' if t.startswith('@') and len(t) > 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return " ".join(new_text)

task = 'hate'
MODEL = f"cardiffnlp/twitter-roberta-base-{task}"

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(MODEL)

# åŠ è½½æ¨¡åž‹
# ä½ çš„æ•°æ®é›†æ ‡ç­¾æ˜¯ 0 (non-hate) å’Œ 1 (hate)ï¼Œæ‰€ä»¥ num_labels=2
model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2)

# --- 2. åŠ è½½ä½ çš„ Kaggle æ•°æ®é›†å¹¶è®¡ç®— Class Weights ---
# (ä½¿ç”¨ä½ æä¾›çš„ kagglehub ä»£ç )

print("Downloading dataset from Kaggle Hub...")
path = kagglehub.dataset_download("vkrahul/twitter-hate-speech")
print(f"Dataset downloaded to: {path}")

data_file_path = Path(path) / "train_E6oV3lV.csv"

if not data_file_path.exists():
    # æ³¨æ„ï¼šåœ¨ä½ çš„åŽŸå§‹ä»£ç ä¸­ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ª 'return'ã€‚
    # å› ä¸ºè¿™ä¸åœ¨ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œæ‰€ä»¥æˆ‘å°†å…¶æ”¹ä¸º 'sys.exit()' æ¥åœæ­¢è„šæœ¬æ‰§è¡Œã€‚
    print(f"Error: Could not find 'train_E6oV3lV.csv' in {path}")
    sys.exit(1) # åœæ­¢è„šæœ¬

print(f"Loading data from {data_file_path}...")
df = pd.read_csv(data_file_path)

df = df[['label', 'tweet']]
df = df.rename(columns={"tweet": "text"})
df = df.dropna(subset=['text'])

print("Calculating class weights...")
class_counts = df['label'].value_counts()
num_samples = len(df)

num_classes = len(class_counts)
weights = num_samples / (num_classes * class_counts)

weights = weights.sort_index() # ç¡®ä¿ç´¢å¼•æŒ‰ 0, 1 æŽ’åº
class_weights = torch.tensor(weights.values, dtype=torch.float32)
print(f"Class Weights (for labels {weights.index.values}): {class_weights}")

# è½¬æ¢ä¸º Hugging Face Dataset
dataset = Dataset.from_pandas(df)

# --- 3. Tokenize æ•°æ®é›† ---
def tokenize_function(examples):
    # é¦–å…ˆåº”ç”¨ä½ çš„ preprocess å‡½æ•°
    processed_texts = [preprocess(t) for t in examples['text']]
    # ç„¶åŽè¿›è¡Œ tokenization
    return tokenizer(processed_texts, padding="max_length", truncation=True, max_length=128)

print("Tokenizing dataset...")
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# é‡å‘½å 'label' åˆ—ä¸º 'labels' ä»¥åŒ¹é…æ¨¡åž‹æœŸæœ›
tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
tokenized_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = split_dataset['train']
eval_dataset = split_dataset['test']

print(f"Train dataset size: {len(train_dataset)}")
print(f"Eval dataset size: {len(eval_dataset)}")

# --- 4. è‡ªå®šä¹‰ Trainer ä»¥ä½¿ç”¨ Class Weights ---
# (è¿™æ˜¯å®žçŽ° "æ³¨æ„ class_weight çš„ä½¿ç”¨" çš„å…³é”®)

class WeightedTrainer(Trainer):
    def __init__(self, *args, class_weights=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights

    def compute_loss(self, model, inputs, return_outputs=False):
        # ä»Žè¾“å…¥ä¸­èŽ·å– 'labels'
        labels = inputs.pop("labels")
        
        # å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        logits = outputs.get("logits")
        
        # å‡†å¤‡ CrossEntropyLoss
        # å°† class_weights ç§»åŠ¨åˆ°æ¨¡åž‹æ‰€åœ¨çš„ device
        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(model.device))
        
        # è®¡ç®—æŸå¤±
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss

# --- 5. è®¾ç½®è®­ç»ƒå‚æ•°å¹¶å¼€å§‹è®­ç»ƒ ---
training_args = TrainingArguments(
    output_dir="./results",               # è¾“å‡ºç›®å½•
    num_train_epochs=3,                 # è®­ç»ƒè½®æ•°
    per_device_train_batch_size=4,      # è®­ç»ƒ batch size
    per_device_eval_batch_size=4,       # éªŒè¯ batch size
    logging_dir='./logs',               # æ—¥å¿—ç›®å½•
    logging_steps=50,                   # å¢žåŠ æ—¥å¿—è®°å½•çš„æ­¥æ•° (å› ä¸ºæ•°æ®é›†æ›´å¤§äº†)
    evaluation_strategy="epoch",        # æ¯è½®ç»“æŸåŽè¿›è¡ŒéªŒè¯
    save_strategy="epoch",              # æ¯è½®ç»“æŸåŽä¿å­˜æ¨¡åž‹
    load_best_model_at_end=True,        # è®­ç»ƒç»“æŸåŽåŠ è½½æœ€ä½³æ¨¡åž‹
    remove_unused_columns=False,        # å¿…éœ€ï¼Œå› ä¸ºæˆ‘ä»¬ä¿ç•™äº† 'labels'
)

# å®žä¾‹åŒ–æˆ‘ä»¬çš„ WeightedTrainer
trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    class_weights=class_weights  # !! åœ¨è¿™é‡Œä¼ å…¥æˆ‘ä»¬çš„æƒé‡ !!
)

print("Starting training...")
trainer.train()
print("Training complete.")

# --- 6. ä½¿ç”¨å¾®è°ƒåŽçš„æ¨¡åž‹è¿›è¡ŒæŽ¨ç† ---
print("\n--- Inference Example (using fine-tuned model) ---")

# ä½¿ç”¨ä½ åŽŸå§‹ä»£ç ä¸­çš„ç¤ºä¾‹æ–‡æœ¬
text = "Good night ðŸ˜Š"
preprocessed_text = preprocess(text)
print(f"Original: '{text}' -> Preprocessed: '{preprocessed_text}'")

encoded_input = tokenizer(preprocessed_text, return_tensors='pt')
# å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡åž‹æ‰€åœ¨çš„ device
encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()}

output = model(**encoded_input)
scores = output[0][0].detach().cpu().numpy()
scores = softmax(scores)

# ä½ çš„ Kaggle æ•°æ®é›†æ ‡ç­¾: 0: non-hate, 1: hate
labels = ['non-hate', 'hate']

ranking = np.argsort(scores)
ranking = ranking[::-1]
for i in range(scores.shape[0]):
    l = labels[ranking[i]]
    s = scores[ranking[i]]
    print(f"{i+1}) {l} {np.round(float(s), 4)}")

# æµ‹è¯•ä¸€ä¸ªä»‡æ¨è¨€è®ºçš„ä¾‹å­
text_hate = "You are a terrible person."
preprocessed_text_hate = preprocess(text_hate)
print(f"\nOriginal: '{text_hate}' -> Preprocessed: '{preprocessed_text_hate}'")

encoded_input = tokenizer(preprocessed_text_hate, return_tensors='pt')
encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()}

output = model(**encoded_input)
scores = output[0][0].detach().cpu().numpy()
scores = softmax(scores)

ranking = np.argsort(scores)
ranking = ranking[::-1]
for i in range(scores.shape[0]):
    l = labels[ranking[i]]
    s = scores[ranking[i]]
    print(f"{i+1}) {l} {np.round(float(s), 4)}")